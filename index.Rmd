---
title: "Computational Musicology"
author: "John Ashley Burgoyne"
date: "2019"
output: 
    flexdashboard::flex_dashboard:
        storyboard: true
        theme: lumen
---

```{r, cache = FALSE}
# In order to use these packages, we need to install flexdashboard, plotly, and Cairo.
library(tidyverse)
library(plotly)
library(spotifyr)
library(compmus)
source('spotify.R')
```

### What kind of storyboard do I need to create?

Your portfolio will be a 5–10-page storyboard in the style of the R package [flexdashboard](https://rmarkdown.rstudio.com/flexdashboard/using.html#storyboards), using data from the [Spotify API](https://developer.spotify.com). Your storyboard should cover the following topics, but note that it is possible (and often desirable) for a single visualisation or tab to cover more than one topic.

  - **Background**: Which tracks did you choose? Why? What questions will the storyboard answer?
  - **Track-level features**: What insights into your corpus can be drawn from Spotify’s track-level features (valence, danceability, etc.)?
  - **Chroma features** [pitch]: What insights into your corpus can be drawn from Spotify’s chroma features?
  - **Loudness** [volume]: What insights into your corpus can be drawn from Spotify’s ‘loudness’ (power) features, either at the track level or the section level?
  - **Timbre features** [timbre]: What insights into your corpus can be drawn from Spotify’s timbre features?
  - **Temporal features** [duration]: What is notable, effective, or ineffective about structural segments, metre, rhythm, or beats in your corpus, as derived from Spotify features?
  - **Classification/regression**: Where explicit labels in your corpus exist, how strong are Spotify features for classification or regression?
  - **Clustering**: Where explicit labels in your corpus are lacking, which Spotify features generate potentially meaningful clusters?
  - **Contribution**: What have you learned from this set of analyses? Who could these conclusions benefit, and how?

Depending on your topic, you may want to start with a text-based opening like this one; alternatively, you could put your most compelling visualisation directly on the first tab and just use the commentary to introduce your corpus and research questions.

This storyboard contains further examples from each week to inspire you. For more detailed code examples from each week, check the [repository page](https://github.com/jaburgoyne/compmus2019) or use the following links for rendered R Markdown files.

- [Week 8](compmus2019-w08.nb.html)
- [Week 9](compmus2019-w09.nb.html)
- [Week 10](compmus2019-w10.nb.html)

***

The grading breakdown for the portfolio is as follows. The rubric was adapted from the Association of American Colleges and Universities (AAC&U) Inquiry and Analysis and Quantitative Literacy [VALUE rubrics](https://www.aacu.org/value-rubrics).

| Component        | Points |
|------------------|:------:|
| Corpus selection |      7 |
| Assumptions      |      7 |
| Representation   |      7 |
| Interpretation   |      7 |
| Analysis         |      7 |
| Presentation     |      7 |

### The Grammys are angrier than the Edisons [track-level features].

```{r}
grammy <- get_playlist_audio_features('digster.fm', '4kQovkgBZTd8h2HCM3fF31')
edison <- get_playlist_audio_features('spotify', '37i9dQZF1DX8mnKbIkppDf')
awards <-
    grammy %>% mutate(playlist = "Grammys") %>%
    bind_rows(edison %>% mutate(playlist = "Edisons"))
angry <-
    awards %>%                   # Start with awards.
    ggplot(                      # Set up the plot.
        aes(
            x = valence,
            y = energy,
            size = loudness,
            colour = mode,
            label = track_name
        )
    ) +
    geom_point(alpha = 0.8) +               # Scatter plot.
    geom_rug(size = 0.1) +       # Add 'fringes' to show data distribution.
    facet_wrap(~ playlist) +     # Separate charts per playlist.
    scale_x_continuous(          # Fine-tune the x axis.
        limits = c(0, 1),
        breaks = c(0, 0.50, 1),  # Use grid-lines for quadrants only.
        minor_breaks = NULL      # Remove 'minor' grid-lines.
    ) +
    scale_y_continuous(          # Fine-tune the y axis in the same way.
        limits = c(0, 1),
        breaks = c(0, 0.50, 1),
        minor_breaks = NULL
    ) +
    scale_colour_brewer(         # Use the Color Brewer to choose a palette.
        type = "qual",           # Qualitative set.
        palette = "Dark2"       # Name of the palette is 'Paired'.
    ) +
    scale_size_continuous(       # Fine-tune the sizes of each point.
        trans = "exp",           # Use an exp transformation to emphasise loud.
        guide = "none"           # Remove the legend for size.
    ) +
    theme_light() +              # Use a simpler theme.
    labs(                        # Make the titles nice.
        x = "Valence",
        y = "Energy",
        colour = "Mode"
    )
ggplotly(angry)
```

***

For this visualisation from Week 7, I took playlists of the pop music presented at the Grammy awards (US) and the Edison awards (NL) in 2019. Using `ggplotly`, the visualisation became interactive.

The *x* axis shows valence and the *y* axis shows Spotify's ‘energy’ feature, which is roughly analogous to the notion of arousal in psychological research on emotion. Under this model, the quadrants of each graph, starting clockwise from the top left, reprsent angry, happy, relaxed, and sad music. The size of each point is proportional to the average volume of the track.

The visualisation shows that in 2019, the pop music at the Grammys was (according to Spotify) rather angrier and rather louder than the music at the Edisons.

### The Tallis Scholars sing Josquin more slowly than La Chapelle Royale -- except for one part [chroma features].

```{r}
tallis <- 
    get_tidy_audio_analysis('2J3Mmybwue0jyQ0UVMYurH') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)
chapelle <- 
    get_tidy_audio_analysis('4ccw2IcnFt1Jv9LqQCOYDi') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)
maria_dist <- 
    compmus_long_distance(
    tallis %>% mutate(pitches = map(pitches, compmus_normalise, 'manhattan')),
    chapelle %>% mutate(pitches = map(pitches, compmus_normalise, 'manhattan')),
    feature = pitches,
    method = 'aitchison')
```

```{r}
maria <- 
    maria_dist %>% 
    mutate(
        tallis = xstart + xduration / 2, 
        chapelle = ystart + yduration / 2) %>% 
    ggplot(
        aes(
            x = tallis,
            y = chapelle,
            fill = d)) + 
    geom_tile(aes(width = xduration, height = yduration)) +
    coord_fixed() +
    scale_x_continuous(
        breaks = c(0, 60, 105, 150, 185, 220, 280, 327), 
        labels = 
            c(
                'Ave Maria',
                'Ave cujus conceptio',
                'Ave cujus nativitas',
                'Ave pia humilitas',
                'Ave vera virginitas',
                'Ave preclara omnibus',
                'O Mater Dei',
                ''),
        ) +
    scale_y_continuous(
        breaks = c(0, 45,  80, 120, 145, 185, 240, 287), 
        labels = 
            c(
                'Ave Maria',
                'Ave cujus conceptio',
                'Ave cujus nativitas',
                'Ave pia humilitas',
                'Ave vera virginitas',
                'Ave preclara omnibus',
                'O Mater Dei',
                ''),
        ) +
    scale_fill_viridis_c(option = 'E', guide = "none") +
    theme_classic() + 
    theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
    labs(x = 'The Tallis Scholars', y = 'La Chapelle Royale')
maria
```

***

This visualisation of two performances of the famous ‘Ave Maria’ setting of Josquin des Prez uses the Aitchison distance between chroma features to show how the two performances align with one another. 

For the first four stanzas, the relationship between the performances is consistent: the Tallis Scholars sing the piece somewhat more slowly than La Chapelle Royale. For the fifth stanza (*Ave vera virginitas*, starting about 3:05 into the Tallis Scholars’ performance and 2:25 into La Chapelle Royale’s), the Tallis Scholars singing faster than La Chapelle Royale, but at the beginning of the sixth stanza (*Ave preclara omnibus*, starting about 3:40 into the the Tallis Scholars’ performance and 3:05 into La Chapelle Royale’s) the Tallis Scholars return to their regular tempo relationship with La Chapelle.

Although the interactive mouse-overs from `ggplotly` are very helpful for understanding heat maps, they are very computationally intensive. Chromagrams and similarity matrices are often better as static images, like the visualisation at left.

Static images can sometimes also be useful to add content to your commentary, like the histogram of Aitchison distances below, labelled with the minimum, first quartile, median, third quartile, and maximum values in the data. You must save the images manually, however, and make sure to export them at a good size.

```{r}
maria_hist <- 
    maria_dist %>% 
    ggplot(aes(x = d)) +
    geom_histogram(binwidth = 0.5) +
    theme_classic() + 
    theme(
        axis.line.y = element_blank(), 
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
    scale_x_continuous(breaks = c(0.7, 4.4, 5.5, 6.8, 14.2)) +
    labs(x = 'Aitchison Distance', y = '')
ggsave("compmus-w08vizb.png", maria_hist, width = 3, height = 2, dpi = 'screen')
```

![](compmus-w08vizb.png)

### Blood, sweat, and tears trying to understand the structure of ‘Blood, Sweat, and Tears’ [chroma and timbre features]

```{r}
bzt <- 
    get_tidy_audio_analysis('5ZLkc5RY1NM4FtGWEd6HOE') %>% 
    compmus_align(bars, segments) %>% 
    select(bars) %>% unnest(bars) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'acentre', norm = 'manhattan')) %>% 
    mutate(
        timbre = 
            map(segments, 
                compmus_summarise, timbre, 
                method = 'mean'))
bztplot <- 
    bind_rows(
        bzt %>% compmus_self_similarity(pitches, 'aitchison') %>% mutate(d = d / max(d), type = "Chroma"),
        bzt %>% compmus_self_similarity(timbre, 'euclidean') %>% mutate(d = d / max(d), type = "Timbre")) %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    facet_wrap(~ type) +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '')
ggplotly(bztplot)
```

***

The two self-similarity matrices at the right, each summarised at the bar level but with axes in seconds, illustrate pitch- and timbre-based self-similarity within Andre Hazes's famous ‘Bloed, Zweet en Tranen’ (2002). Both are necessary to understand the structure of the song. The chroma-based matrix picks up the five presentations of the chorus very clearly but mostly misses the poignant changes in texture during the three verses. These changes are very visible in the timbre-based matrix, especially the third verse. The timbre-based matrix also illustrates the unbalanced song structure, climaxing about halfway through and thereafter simply repeating the chorus until the fade-out. The closing guitar solo is faintly visible in the top-right corner.

### Truck-driver modulations in the Year 2525 [tonal analysis]

```{r}
circshift <- function(v, n) {if (n == 0) v else c(tail(v, n), head(v, -n))}
                                    
major_key <- 
    c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
    c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

key_templates <-
    tribble(
        ~name    , ~template,
        'Gb:maj', circshift(major_key,  6),
        'Bb:min', circshift(minor_key, 10),
        'Db:maj', circshift(major_key,  1),
        'F:min' , circshift(minor_key,  5),
        'Ab:maj', circshift(major_key,  8),
        'C:min' , circshift(minor_key,  0),
        'Eb:maj', circshift(major_key,  3),
        'G:min' , circshift(minor_key,  7),
        'Bb:maj', circshift(major_key, 10),
        'D:min' , circshift(minor_key,  2),
        'F:maj' , circshift(major_key,  5),
        'A:min' , circshift(minor_key,  9),
        'C:maj' , circshift(major_key,  0),
        'E:min' , circshift(minor_key,  4),
        'G:maj' , circshift(major_key,  7),
        'B:min' , circshift(minor_key, 11),
        'D:maj' , circshift(major_key,  2),
        'F#:min', circshift(minor_key,  6),
        'A:maj' , circshift(major_key,  9),
        'C#:min', circshift(minor_key,  1),
        'E:maj' , circshift(major_key,  4),
        'G#:min', circshift(minor_key,  8),
        'B:maj' , circshift(major_key, 11),
        'D#:min', circshift(minor_key,  3))

twenty_five <- 
    get_tidy_audio_analysis('5UVsbUV0Kh033cqsZ5sLQi') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'acentre', norm = 'manhattan')) %>% 
    compmus_match_pitch_template(key_templates, 'aitchison', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '', fill = 'Distance')
ggplotly(twenty_five)
```

***
    
The keygram at the left shows the two modulations in Zager and Evans's 'In the Year 2525' (1969). The piece is segmented according to Spotify's estimates, and the distances represented are Aitchison distances from Spotify's chroma vectors to the original Krumhansl--Kessler key profiles (1990). 

The piece does not follow common-era tonal conventions, and the key estimates are blurry. The move from G$\sharp$ minor to A minor about a minute and a half into the song is correctly estimated, despite the high spillage into related keys. The second modulation, to B$\flat$ minor, is misunderstood as F minor. The sparser texture two-and-a-half minutes into the song throws off the key-finding algorithm seriously.
    
```

